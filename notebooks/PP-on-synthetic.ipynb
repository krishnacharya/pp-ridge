{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MayjuI8twv0v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWDG9-xGxKY6"
   },
   "source": [
    "NORMAL DIST -- SYNTHETIC DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgSZtbwJy8VO"
   },
   "source": [
    "Generating the synthetic data, privacy specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d1uQKD7Ay7yf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 10) (250, 10) (750,) (250,)\n",
      "Counter({1.0: 500, 0.1: 250})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize, minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = 1000 # synthetic dataset size\n",
    "d = 10 # dimensionality\n",
    "\n",
    "theta = np.random.uniform(0, 10, size=d)\n",
    "lamb = 0.5 # norm penalizer parameter for ridge\n",
    "X, y = generate_linear_data(n = N, theta = theta, sigma=0.1)\n",
    "\n",
    "# Our theory is for all features within the L2 unit ball, and y's in [0,1]\n",
    "X = normalize(X, norm='l2') # each row is L2 normalized\n",
    "y = minmax_scale(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 21)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "N_train, N_test = len(X_train), len(X_test)\n",
    "\n",
    "epsilons = np.zeros(N_train)# training datas agents privacy levels, want 3 privacy levels\n",
    "epsilons[:N_train//3] = 0.1\n",
    "epsilons[N_train//3 : 2 * N_train//3] = 0.5 # change to 0.5\n",
    "epsilons[2 * N_train//3 : ] = 1.0\n",
    "from collections import Counter\n",
    "print(Counter(epsilons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((750,), (750, 10), 750, 250)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilons.shape, X_train.shape, N_train, N_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-ObrbcwxoKg"
   },
   "source": [
    "### PERSONALIZED PRIVACY, (our reweighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WagHK29kxPXD",
    "outputId": "deadcd31-5348-4e75-e476-19e3bf40fe31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta for pp 54.36553006146873\n",
      "unweighted_train_usingour_pp 0.14335377539478197 0.015552564697300625\n",
      "unweighted_test_usingour_pp 0.14304419657907377 0.015541359198146325\n"
     ]
    }
   ],
   "source": [
    "tot_epsilon = np.sum(epsilons)\n",
    "weights_pp = epsilons / tot_epsilon # weights used in the ridge regression for personalized privacy\n",
    "\n",
    "sol_exact_ridge_pp = weighted_rls_solution(weights_pp, X_train, y_train, lamb) # weighted ridge on training set\n",
    "# print(\"pluggin exact soln back into weighted ridge\", evaluate_weighted_rls_objective(sol_exact_ridge_pp, weights_pp, X, y, lamb))\n",
    "beta_pp = compute_beta(lamb, tot_epsilon)\n",
    "print(\"beta for pp\", beta_pp)\n",
    "# Do runs, in each calculate loss on unweighted train, unweighted test set loss of the private estimator; 1000 runs for randomness in L2 laplce dp noise\n",
    "runs = 100\n",
    "unweighted_train = []\n",
    "unweighted_test = []\n",
    "uniform_weight_train = np.ones(N_train) / N_train\n",
    "uniform_weight_test = np.ones(N_test) / N_test\n",
    "# weighted_erm = []\n",
    "for _ in range(runs):\n",
    "  theta_hat_pp = compute_private_estimator(sol_exact_ridge_pp, beta_pp) # exact solution on weighted training + noise\n",
    "  unweighted_train.append(evaluate_weighted_rls_objective(theta_hat_pp, uniform_weight_train, X_train, y_train, lamb))\n",
    "  unweighted_test.append(evaluate_weighted_rls_objective(theta_hat_pp, uniform_weight_test, X_test, y_test, lamb))\n",
    "  # weighted_erm.append(evaluate_weighted_rls_objective(theta_hat_pp, weights_pp, X, y, lamb))\n",
    "print(\"unweighted_train_usingour_pp\", np.mean(unweighted_train), np.std(unweighted_train))\n",
    "print(\"unweighted_test_usingour_pp\", np.mean(unweighted_test), np.std(unweighted_test))\n",
    "# print(\"weighted_erm_using_privateestimator\", np.mean(weighted_erm), np.std(weighted_erm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHBDXUwkywue"
   },
   "source": [
    "### *Not* PERSONALIZED PRIVACY, using our framework\n",
    "\n",
    "  Epsilon for all agents set to min of epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6NB2PeTxccZ",
    "outputId": "18019a39-8dc0-4de1-c43c-b8df95b1e3fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta for not 7.766504294495533\n",
      "unweighted_train_usingour_pp 1.1799538033701387 0.7125864035430834\n",
      "unweighted_test_usingour_pp 1.1791270047252367 0.7117552043429053\n"
     ]
    }
   ],
   "source": [
    "tot_epsilon = min(epsilons) * N_train # each agents privacy set to min epsilon\n",
    "uniform_weight_train = np.ones(N_train) / N_train\n",
    "uniform_weight_test = np.ones(N_test) / N_test\n",
    "\n",
    "soln_ridge = weighted_rls_solution(uniform_weight_train, X_train, y_train, lamb)\n",
    "# print(\"pluggin exact soln back into unweighted ridge\", evaluate_weighted_rls_objective(sol_exact_ridge_npp, weights_npp, X, y, lamb))\n",
    "beta = compute_beta(lamb, tot_epsilon)\n",
    "print(\"beta for not\", beta)\n",
    "# to loop the part below\n",
    "runs = 100\n",
    "unweighted_train = []\n",
    "unweighted_test = []\n",
    "for _ in range(runs):\n",
    "  theta_hat = compute_private_estimator(soln_ridge, beta)\n",
    "  unweighted_train.append(evaluate_weighted_rls_objective(theta_hat, uniform_weight_train, X_train, y_train, lamb))\n",
    "  unweighted_test.append(evaluate_weighted_rls_objective(theta_hat, uniform_weight_test, X_test, y_test, lamb))\n",
    "  # weighted_erm.append(evaluate_weighted_rls_objective(theta_hat_npp, weights_npp, X, y, lamb))\n",
    "print(\"unweighted_train_usingour_pp\", np.mean(unweighted_train), np.std(unweighted_train))\n",
    "print(\"unweighted_test_usingour_pp\", np.mean(unweighted_test), np.std(unweighted_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Privacy using sampling mechanism from Jorgensen\n",
    "\n",
    "- First subsample dataset points based on epsilons provided by each individual (and a threshold t) ,\n",
    "- then run DP with threshold t on the sampled data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 10) (750,) (750,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, epsilons.shape)\n",
    "mask = dataset_mask_jorgensen(epsilons, max(epsilons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unweighted_erm_using_privateestimator 0.1444019479448102 0.014186907704786984\n",
      "weighted_erm_using_privateestimator 0.1439455257338352 0.01429883656338262\n"
     ]
    }
   ],
   "source": [
    "thresh = max(epsilons) #global threshold used in jorgensen sampling\n",
    "# to loop the part below\n",
    "runs = 100\n",
    "uniform_weight_train = np.ones(N_train) / N_train\n",
    "uniform_weight_test = np.ones(N_test) / N_test\n",
    "\n",
    "unweighted_train = []\n",
    "unweighted_test = []\n",
    "for _ in range(runs):\n",
    "  mask = dataset_mask_jorgensen(epsilons, thresh) # which datapoint in X_train, y_train to mask, shape (N_train)\n",
    "  X_samp = X_train[mask.astype(bool)]\n",
    "  y_samp = y_train[mask.astype(bool)]\n",
    "  N_samp = len(y_samp)\n",
    "  unif_weight_samp = np.ones(N_samp) / N_samp\n",
    "  tot_epsilon = thresh * N_samp # Use global threshold epsilon as privacy level for each sampled datapoint\n",
    "  # now do DP with global threshold thresh, on the sampled data, using our sensitivity calculations\n",
    "  theta_bar = weighted_rls_solution(unif_weight_samp, X_samp, y_samp, lamb) # unweighted soln with sampled data\n",
    "  beta = compute_beta(lamb, tot_epsilon)\n",
    "  theta_hat = compute_private_estimator(theta_bar, beta)\n",
    "  unweighted_train.append(evaluate_weighted_rls_objective(theta_hat, uniform_weight_train, X_train, y_train, lamb))\n",
    "  unweighted_test.append(evaluate_weighted_rls_objective(theta_hat, uniform_weight_test, X_test, y_test, lamb))\n",
    "print(\"unweighted_erm_using_privateestimator\", np.mean(unweighted_train), np.std(unweighted_train)) # WE care about low values here!\n",
    "print(\"weighted_erm_using_privateestimator\", np.mean(unweighted_test), np.std(unweighted_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda, n, d"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "local_multigroup",
   "language": "python",
   "name": "local_multigroup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
